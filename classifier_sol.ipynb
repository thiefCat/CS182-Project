{"cells":[{"cell_type":"markdown","metadata":{"id":"quDoKylMIfLh"},"source":["# PointNet\n","\n","In this assignment, students will implement the PointNet Classification and PointNet Segmentation modules. Students can learn about point cloud data and pre-processing, implement shared MLP and Joint Alignment Network, tune the alpha value to observe the accuracy change, and write the code for the loss function to understand better what the network tries to learn.\n","\n","This homework is based on the paper [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)\n","\n","This homework refers to the article [Deep Learning on Point clouds](https://towardsdatascience.com/deep-learning-on-point-clouds-implementing-pointnet-in-google-colab-1fd65cd3a263)"]},{"cell_type":"markdown","metadata":{"id":"tvaKPCPmD0xf"},"source":["### Overview\n","In this assignment, you will implement the PointNet Classification and PointNet Segmentation modules. In this homework, you will learn about point cloud data and pre-processing, you will implement shared MLP and Joint Alignment Network, tune the hyperparameters to observe the accuracy change, and write the code for the loss function to understand better what the network tries to learn."]},{"cell_type":"markdown","metadata":{"id":"W14I064Vcirk"},"source":["## Preparing the point cloud data"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":342,"status":"ok","timestamp":1683251231015,"user":{"displayName":"陈君至","userId":"05086203482770768833"},"user_tz":420},"id":"_gwT2_VukiXd"},"outputs":[],"source":["import math\n","import random\n","import os\n","import numpy as np\n","np.random.seed(7)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import scipy.spatial.distance\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from tqdm.notebook import trange, tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","\n","import plotly.graph_objects as go\n","import plotly.express as px"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":670,"status":"ok","timestamp":1683251232419,"user":{"displayName":"陈君至","userId":"05086203482770768833"},"user_tz":420},"id":"J_Ry3IPWHkIt","outputId":"34a0f1fb-b6b2-42b7-833a-3c41d182d413"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CjRO5yJaJAzz","outputId":"baa20b4b-150f-46ae-c2e6-08bdb511e2b2","executionInfo":{"status":"ok","timestamp":1683251243730,"user_tz":420,"elapsed":11314,"user":{"displayName":"陈君至","userId":"05086203482770768833"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: path.py in /usr/local/lib/python3.10/dist-packages (12.5.0)\n","Requirement already satisfied: path in /usr/local/lib/python3.10/dist-packages (from path.py) (16.6.0)\n"]}],"source":["!pip install path.py;\n","from path import Path"]},{"cell_type":"markdown","metadata":{"id":"Ftr9exUqJV_e"},"source":["Download the dataset\n","\n","When you download a file using **wget** in Google Colab, the file is saved to the virtual machine's temporary storage associated with the current runtime session. By default, the downloaded file will be saved in the current working directory, which is /content.\n","\n","The downloaded file will be lost when the runtime is disconnected."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4reqd-VdJUzk","outputId":"fa9a9d21-0204-4810-9552-f7359c2349f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","--2023-05-05 01:47:23--  http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n","Resolving 3dvision.princeton.edu (3dvision.princeton.edu)... 128.112.136.74\n","Connecting to 3dvision.princeton.edu (3dvision.princeton.edu)|128.112.136.74|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip [following]\n","--2023-05-05 01:47:23--  https://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n","Connecting to 3dvision.princeton.edu (3dvision.princeton.edu)|128.112.136.74|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 473402300 (451M) [application/zip]\n","Saving to: ‘ModelNet10.zip.1’\n","\n","ModelNet10.zip.1    100%[===================>] 451.47M  13.1MB/s    in 38s     \n","\n","2023-05-05 01:48:01 (12.0 MB/s) - ‘ModelNet10.zip.1’ saved [473402300/473402300]\n","\n","replace ModelNet10/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["#@title Download the dataset\n","%cd /content\n","!wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n","!unzip -q ModelNet10.zip;"]},{"cell_type":"markdown","metadata":{"id":"YNNGIrIuPZj7"},"source":["The dataset contains 10 classes, labeled from 0, 1, ..., 9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKsEgCl-Mrrw"},"outputs":[],"source":["path_unsampled = Path(\"ModelNet10\")\n","# read all the directories in ModelNet10\n","folders = [dir for dir in sorted(os.listdir(path_unsampled)) if os.path.isdir(path_unsampled/dir)]\n","classes = {folder: i for i, folder in enumerate(folders)};\n","classes"]},{"cell_type":"markdown","metadata":{"id":"iwoTSsGZQHcE"},"source":["### Understand the data"]},{"cell_type":"markdown","metadata":{"id":"K2sVKAhzQGPn"},"source":["This dataset consists of **.off** files that contain meshes represented by **vertices** and **triangular faces**\n","\n","Take the data in the file 'chair/train/chair_0001.off' as an example:\n","\n","- The first line is a symbol **OFF** representing the file is an .off file.\n","- The second line has three numbers. The first number is the number of vertices which is 2382 and the second number is the number of faces which is 2234.\n","- The following 2382 lines are coordinates of vertices in the space.\n","- The last 2234 lines are faces in the space. In each line, the first number is 3, the following 3 numbers are three vertices of a triangle."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23ypF-0mQfPr"},"outputs":[],"source":["#@title display the first 6 lines in the file\n","file_path = path_unsampled/\"chair/train/chair_0001.off\"\n","!sed -n '1,6p' \"$file_path\" | cat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9q8RCpFUIoW"},"outputs":[],"source":["#@title display the first line 3000-3005 in the file\n","!sed -n '3000,3005p' \"$file_path\" | cat"]},{"cell_type":"markdown","metadata":{"id":"bMJW6NG8WzrX"},"source":["### Read the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4Se0yARPk0V"},"outputs":[],"source":["def read_off(file):\n","  '''\n","  return: verts: List[List]\n","          faces: List[List]\n","  '''\n","  if 'OFF' != file.readline().strip():\n","    raise('Not a valid OFF header')\n","  # read the first line\n","  n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n","  # read vertices and facess\n","  verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n","  faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n","  return verts, faces\n","\n","def read_sampled_off(file):\n","  if 'OFF' != file.readline().strip():\n","    raise('Not a valid OFF header')\n","  file.readline()\n","  file.readline()\n","  n_points = tuple([int(s) for s in file.readline().strip().split(' ')])[0]\n","  file.readline()\n","  points = [[float(s) for s in file.readline().strip().split(' ')] for i in range(n_points)]\n","  return points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvVeYZYqYd6C"},"outputs":[],"source":["# read vertices and faces of file chair_0001.off\n","with open(path_unsampled/\"chair/train/chair_0001.off\", 'r') as f:\n","  verts, faces = read_off(f)"]},{"cell_type":"markdown","metadata":{"id":"15ZPvJ3KZNwT"},"source":["### Visualize the *data*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z3kyPZ1Zyt2"},"outputs":[],"source":["def visualize_rotate(data):\n","  x_eye, y_eye, z_eye = 1.25, 1.25, 0.8\n","  frames=[]\n","\n","  def rotate_z(x, y, z, theta):\n","    w = x+1j*y\n","    return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z\n","\n","  for t in np.arange(0, 10.26, 0.1):\n","    xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)\n","    frames.append(dict(layout=dict(scene=dict(camera=dict(eye=dict(x=xe, y=ye, z=ze))))))\n","  fig = go.Figure(data=data,\n","                  layout=go.Layout(updatemenus=[dict(type='buttons',\n","                                    showactive=False,\n","                                    y=1,\n","                                    x=0.8,\n","                                    xanchor='left',\n","                                    yanchor='bottom',\n","                                    pad=dict(t=45, r=10),\n","                                    buttons=[dict(label='Play',\n","                                            method='animate',\n","                                            args=[None, dict(frame=dict(duration=50, redraw=True),\n","                                                            transition=dict(duration=0),\n","                                                            fromcurrent=True,\n","                                                            mode='immediate')]\n","                                                  )\n","                                            ]\n","                                      )\n","                                  ]\n","                                  ),\n","                  frames=frames)\n","  return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9KlHHnjZrFu"},"outputs":[],"source":["def pcshow(xs,ys,zs):\n","    data=[go.Scatter3d(x=xs, y=ys, z=zs,\n","                                   mode='markers')]\n","    fig = visualize_rotate(data)\n","    fig.update_traces(marker=dict(size=2,\n","                      line=dict(width=2,\n","                      color='DarkSlateGrey')),\n","                      selector=dict(mode='markers'))\n","    fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIUsxsdlaEpA"},"outputs":[],"source":["#@title visualize vertices in 3d space\n","x,y,z = np.array(verts).T\n","pcshow(x,y,z)"]},{"cell_type":"markdown","metadata":{"id":"x79YjxdzaiOH"},"source":["### Transform the data into real point cloud"]},{"cell_type":"markdown","metadata":{"id":"OoQMuqoPdOzo"},"source":["As you can see in the visualization, the point cloud for vertices does not really look like a chair. \n","\n","This is because we haven't utilized faces information yet. Remember, we want the point cloud for an object to be evenly distributed among the faces of the object.\n","\n","In the following PointCloudData class, we will do the following: \n","- Sample points from faces of each object\n","- Normalize the data\n","- Add rotation to the pointcloud\n","- Add random gaussian noise to the point cloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8N4bx3Wgql7t"},"outputs":[],"source":["# The class that handles reading data\n","class PointCloudData(Dataset):\n","    def __init__(self, root_dir, folder=\"train\", sampled=False, k=1024, norm=True, rotation=True, noise=True):\n","      self.root_dir = root_dir\n","      self.sampled = sampled\n","      self.k = k\n","      folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n","      self.classes = {folder: i for i, folder in enumerate(folders)}\n","      self.files = []\n","      self.norm = norm\n","      self.rotation = rotation\n","      self.noise = noise\n","      for directory in self.classes.keys():\n","        new_dir = root_dir/Path(directory)/folder\n","        for file in os.listdir(new_dir):\n","          if file.endswith('.off'):\n","            data = {}\n","            data['path'] = new_dir/file\n","            data['category'] = directory\n","            self.files.append(data)\n","\n","    def __len__(self):\n","      return len(self.files)\n","    \n","    def sample(self, verts, faces):\n","      ########################################################################\n","      # TODO: sample k points among all faces with weights by their areas\n","      # i.e., we want larger triangles to have more points to be sampled in \n","      # the plane of triangle with larger area. The sampled points in each face \n","      # should be proportional to its area\n","      # Input: self.k: number of samples\n","      #        verts: List[List] contains all vertices of an object\n","      #        faces: List[List] contains the indices of vertices of each triangle\n","      # Return: numpy array of shape [k, 3]\n","      # hint: 1. read the following two functions: triangle_area and sample_point\n","      #       2. random.choices can be helpful\n","      ########################################################################\n","      areas = np.zeros((len(faces)))\n","      verts = np.array(verts)\n","      for i in range(len(areas)):\n","        areas[i] = self.triangle_area(verts[faces[i][0]], verts[faces[i][1]], verts[faces[i][2]])\n","      # sample k points among all faces with weights by their areas\n","      sampled_faces = random.choices(faces, weights=areas, k=self.k)\n","      pointcloud = np.zeros((self.k, 3))\n","      for i in range(len(sampled_faces)):\n","        pointcloud[i] = self.sample_point(verts[sampled_faces[i][0]],\n","                                          verts[sampled_faces[i][1]],\n","                                          verts[sampled_faces[i][2]])\n","      return pointcloud\n","      ########################################################################\n","\n","    def triangle_area(self, pt1, pt2, pt3):\n","      '''\n","      input: Coordinates of three vertices of a triangle\n","      output: The area of the triangle\n","      '''\n","      side_a = np.linalg.norm(pt1 - pt2)\n","      side_b = np.linalg.norm(pt2 - pt3)\n","      side_c = np.linalg.norm(pt3 - pt1)\n","      s = 0.5 * (side_a + side_b + side_c)\n","      return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5 \n","\n","\n","    def sample_point(self, pt1, pt2, pt3):\n","      '''\n","      input: Coordinates of three vertices of a triangle\n","      output: The coordinate of a randomly sampled point in the triangle\n","      '''\n","      s, t = sorted([np.random.random(), np.random.random()])\n","      f = lambda i: s * pt1[i] + (t-s) * pt2[i] + (1-t) * pt3[i]\n","      return [f(0), f(1), f(2)]\n","\n","    def __getitem__(self, idx):\n","      path = self.files[idx]['path']\n","      # print(path)\n","      category = self.files[idx]['category']\n","\n","      if self.sampled:\n","        with open(path, 'r') as f:\n","          pointcloud = np.array(read_sampled_off(f)) \n","      else:\n","        with open(path, 'r') as f:\n","          verts, faces = read_off(f)\n","          pointcloud = self.sample(verts, faces)\n","\n","      if self.normalize:\n","        pointcloud = self.normalize(pointcloud)\n","      if self.rotation:\n","        pointcloud = self.rotate(pointcloud)\n","      if self.noise:\n","        pointcloud = self.add_noise(pointcloud)\n","      \n","\n","      return {'pointcloud': torch.from_numpy(pointcloud), \n","              'category': self.classes[category]}\n","\n","    def normalize(self, pointcloud):\n","      ########################################################################\n","      # TODO: Given an unnormalized pointcloud, return the normalized pointclud.\n","      # we want the normalized_pointcloud to have the following property:\n","      # 1. The mean of all points in pointcloud is (0,0,0)\n","      # 2. All the point in the pointcloud are in the unit sphere with center (0,0,0)\n","      # \n","      # Input: pointcloud: numpy array of shape [k, 3] where k is \n","      #        the number of sampled points of the pointcloud\n","      # Return: norm_pointcloud: numpy array of shape [K, 3] which is normalized\n","      ########################################################################\n","      pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n","      norm_pointcloud = pointcloud / np.max(np.linalg.norm(pointcloud, axis=1))\n","      ########################################################################\n","      return norm_pointcloud\n","\n","    def rotate(self, pointcloud):\n","      theta = random.random() * 2. * math.pi # rotation angle\n","      rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n","                          [ math.sin(theta),  math.cos(theta),    0],\n","                          [0,                             0,      1]])\n","\n","      pointcloud = rot_matrix.dot(pointcloud.T).T\n","      return pointcloud\n","\n","    def add_noise(self, pointcloud):\n","      noise = np.random.normal(0, 0.02, (pointcloud.shape))\n","      noisy_pointcloud = pointcloud + noise\n","      return noisy_pointcloud\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"XoK8YzmHd5yx"},"source":["Test your implementation\n","\n","The sampling result should look like this: \n","\n","<img src=\"https://github.com/58191554/PointNet-Project/blob/main/img/desk_points.png?raw=true\" alt=\"chair\" width=\"300\" height=\"200\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNfuAoIddDP1"},"outputs":[],"source":["# Testing your sample function, please visualize your sampled data\n","train_dataset_grade = PointCloudData(path_unsampled, folder='train', sampled=False, k=3000, \n","                               norm=False, rotation=False, noise=False)\n","with open(path_unsampled/\"chair/train/chair_0001.off\", 'r') as f:\n","  verts, faces = read_off(f)\n","pointcloud_grade = train_dataset_grade.sample(verts, faces)\n","pcshow(*pointcloud_grade.T)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLSjD4qbnK5O"},"outputs":[],"source":["# Testing normalize\n","torch.manual_seed(89)\n","correct1 = np.array([[-0.65969586, -0.61102563, -0.4375487],\n","                     [-0.17281342,  0.41985238,  0.22575168],\n","                     [ 0.57366514,  0.21148148, -0.09146313],\n","                     [-0.21824756, -0.01621679,  0.14690676],\n","                     [ 0.55209243,  0.02533382, -0.38807073],\n","                     [-0.07500052, -0.02942534,  0.5444241 ]])\n","pointcloud = torch.randn(6,3).numpy()\n","res1 = train_dataset_grade.normalize(pointcloud)\n","print('Testing normalize:')\n","if not np.allclose(res1, correct1, rtol=1e-03):\n","  print(\"Error\")\n","  print(\"Your answer is: \", res1)\n","  print(\"The expected answer is: \", correct1)\n","else:\n","  print('passed')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Au9Nrj9nhTY"},"outputs":[],"source":["#@title read the data and visualize the data\n","train_dataset_visualize = PointCloudData(path_unsampled, folder='train', sampled=False, k=3000, \n","                               norm=True, rotation=True, noise=False)\n","test_dataset_visualize = PointCloudData(path_unsampled, folder='test', sampled=False, k=3000, \n","                              norm=True, rotation=False, noise=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Zwy3Xil-vVc"},"outputs":[],"source":["inv_classes = {i: cat for cat, i in train_dataset_visualize.classes.items()};\n","inv_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxIml-_u-yVc"},"outputs":[],"source":["print('Train dataset size: ', len(train_dataset_visualize))\n","print('Test dataset size: ', len(test_dataset_visualize))\n","print('Number of classes: ', len(train_dataset_visualize.classes))\n","print('Sample pointcloud shape: ', train_dataset_visualize[0]['pointcloud'].size())\n","print('Sample pointcloud dtype: ', train_dataset_visualize[0]['pointcloud'].dtype)\n","print('Class: ', inv_classes[train_dataset_visualize[0]['category']])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoyvV29ca9NH"},"outputs":[],"source":["item_visualize = train_dataset_visualize[820] # feel free to change the index to visualize different pointcloud\n","pointcloud = item_visualize['pointcloud']\n","category = item_visualize['category']\n","print('Class:', inv_classes[category])\n","pcshow(*pointcloud.T)"]},{"cell_type":"markdown","metadata":{"id":"-rTXdwVRCRtX"},"source":["### Download the pre-sampled data"]},{"cell_type":"markdown","metadata":{"id":"8tNSnPUYBf0X"},"source":["During training, since sampling data from faces takes a lot of time. To save time, we have pre-sampled the data for you. We sampled 1024 points in each pointcloud as in the paper. Now we download the pre-sampled data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nFaok0qx5zX"},"outputs":[],"source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1nt-KtkxvpfJVwBZL51uEnwG_khK67M6u' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1nt-KtkxvpfJVwBZL51uEnwG_khK67M6u\" -O pre_sample.zip && rm -rf /tmp/cookies.txt\n","!unzip -q pre_sample.zip;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RMLAizYDgCM"},"outputs":[],"source":["path = Path(\"off_points\")\n","trainDataset = PointCloudData(path, folder='train', sampled=True, norm=True, rotation=True, noise=True)\n","testDataset = PointCloudData(path, folder='test', sampled=True, norm=True, rotation=False, noise=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"im2-dscpFVW4"},"outputs":[],"source":["inv_classes = {i: cat for cat, i in trainDataset.classes.items()};\n","inv_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUAr3XNFF9_F"},"outputs":[],"source":["print('Train dataset size: ', len(trainDataset))\n","print('Test dataset size: ', len(testDataset))\n","print('Number of classes: ', len(trainDataset.classes))\n","print('Sample pointcloud shape: ', trainDataset[0]['pointcloud'].shape)\n","print('Sample pointcloud dtype: ', trainDataset[0]['pointcloud'].dtype)\n","print('Class: ', inv_classes[trainDataset[0]['category']])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sw1_t_3OEKIE"},"outputs":[],"source":["item = trainDataset[423]\n","pointcloud = item['pointcloud']\n","category = item['category']\n","pcshow(*pointcloud.T)\n","print(inv_classes[category])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQr9eIcejCoI"},"outputs":[],"source":["trainLoader = DataLoader(dataset=trainDataset, batch_size=32, shuffle=True)\n","testLoader = DataLoader(dataset=testDataset, batch_size=64)"]},{"cell_type":"markdown","metadata":{"id":"5sfru2zxkjRQ"},"source":["## Model Structure"]},{"cell_type":"markdown","metadata":{"id":"KTYp7JC56U3L"},"source":["The first transformation network is a mini-PointNet that\n","takes raw point cloud as input and regresses to a 3 × 3\n","matrix. It’s composed of a shared MLP(64, 128, 1024)\n","network (with layer output sizes 64, 128, 1024) on each\n","point, a max pooling across points and two fully connected\n","layers with output sizes 512, 256.\n","#### Understanding Shared MLP\n","An important concept in the paper is Shared-MLP. Shared-MLP takes an input of shape [n, m] and returns a [n, k] matrix. The reason that it's called Shared-MLP is that it does the same as what nn.Linear does except that the matrix used to transform it is the same across the layer. In the following problem, you can understand that Shared-MLP is essentially the same as convolution with kernel size 1.\n","\n","Implement the one-layer shared MLP and compare it with Conv1d to understand what Shared-MLP is and how Shared-MLP works.\n","- Implement the Matrix-based Shared-MLP\n","- Check the Matrix-based Shared-MLP has the same numbers of parameter as Conv1d\n","- Check the forward output of them are the same.  \n","\n","After that, you are free to use conv1d as shared-MLP in latter design"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xd14XvC57LIF"},"outputs":[],"source":["class SharedMLPLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(SharedMLPLayer, self).__init__()\n","        torch.manual_seed(42)        \n","        ####################################################################\n","        # TODO: Implement the __init__ function\n","        # input: input_dim is the feature dimension of the input x\n","        #        output_dim is the feature dimension of the output \n","        # \n","        # hint: initialize the random params with given torch.randn.\n","        ####################################################################\n","        self.sharedMatrix = nn.Parameter(torch.randn(input_dim, output_dim))\n","        ########################## END #####################################\n","    \n","    def forward(self, x):\n","        ####################################################################\n","        # input: a tensor of shape [N, input_dim]\n","        # output a tensor of shape [N, output_dim]\n","        ####################################################################\n","        out = x @ self.sharedMatrix\n","        ########################## END #####################################\n","        return out\n","\n","k = 3\n","c = 64\n","sharedMLP = SharedMLPLayer(k, c)\n","\n","# check the weight shape of sharedMLP\n","for name, param in sharedMLP.named_parameters():\n","    print(name, param.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUWnOhDj7bjv"},"outputs":[],"source":["# An equivalent Conv1d layer implemend, and will be used to compare later\n","class Conv1d_layer(nn.Module):\n","    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=False):\n","        super(Conv1d_layer, self).__init__()\n","        self.conv1d_layer = nn.Conv1d(in_channel, out_channel, kernel_size, stride, bias=False)\n","\n","        torch.manual_seed(42)\n","        self.conv_weights = torch.randn(in_channel, out_channel)\n","        conv_weights_ = self.conv_weights.t()  \n","        conv_weights_ = torch.unsqueeze(conv_weights_, dim=-1)\n","        self.conv1d_layer.weight.data = conv_weights_\n","\n","    def forward(self, x):\n","        out = self.conv1d_layer(x)\n","        out = torch.squeeze(out)  \n","        out = torch.transpose(out, 0, 1)  \n","        return out\n","\n","in_channel = 3\n","out_channel = 64\n","kernel_size = 1\n","stride = 1\n","\n","# Check the shape of conv1d_layer\n","conv1d_layer = Conv1d_layer(in_channel, out_channel, kernel_size, stride=1, bias=False)\n","for name, param in conv1d_layer.named_parameters():\n","    print(name, param.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEUc8X9b7c6M"},"outputs":[],"source":["# Check the Matrix-based Shared-MLP has the same weights of Conv1d\n","print(\"Check the parameter number of sharedMLP and conv1d_layer is the same\")\n","if not torch.allclose(conv1d_layer.conv_weights, sharedMLP.sharedMatrix, atol=1e-5):\n","    print(\"Error\")\n","    print(\"Your sharedMLP weight = \\n\", sharedMLP.sharedMatrix.shape)\n","    print(\"Expected sharedMLP weight should be equal to conv1d_layer \\n\", conv1d_layer.conv_weights.shape)\n","    print(\"Difference = \", torch.norm(conv1d_layer.conv_weights-sharedMLP.sharedMatrix).item())\n","assert torch.allclose(conv1d_layer.conv_weights, sharedMLP.sharedMatrix, atol=1e-5), \"conv_weights and sharedMLP.sharedMatrix are not equal\"\n","print(\"passed\")\n","\n","\n","print(\"\\nCheck the forward output of the sharedMLP and conv1d_layer is the same\")\n","x0 = torch.randn(5, 3)\n","y0 = sharedMLP(x0)\n","z0 = conv1d_layer(torch.transpose(torch.unsqueeze(x0, 0), 1, 2))\n","if not torch.allclose(y0, z0, atol=1e-5):\n","    print(\"Error\")\n","    print(\"y0 = \", y0)\n","    print(\"z0 = \", z0)\n","    print(\"Difference = \", torch.norm(y0-z0).item())\n","assert torch.allclose(y0, z0, atol=1e-5), \"y0 and z0 are not equal\"\n","print(\"passed\")"]},{"cell_type":"markdown","metadata":{"id":"ed9XVq0alUiN"},"source":["#### T-Net\n","The T-Net module is a type of Spatial Transformer Network (STN) that learns a kxk transformation matrix for a given point cloud, which is then used to transform the point cloud to a canonical pose. It consists of two parts: a convolutional network and a fully connected network. The convolutional network maps the input point cloud to a feature space, consisting of a series of convolutional layers with batch normalization and ReLU activation. The fully connected network takes the feature space and learns the transformation matrix, consisting of fully connected layers with batch normalization and ReLU activation. Finally, the T-Net applies the transformation matrix to the input point cloud to transform it to a canonical pose.\n","![T-net](https://github.com/58191554/PointNet-Project/blob/main/img/T-net_pipeline.drawio.png?raw=true)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfLTlMxumqPp"},"outputs":[],"source":["class Tnet(nn.Module):\n","    \"\"\"\n","    T-Net is a type of spatial transformer network (STN) that learns a kxk transformation matrix\n","    for a given point cloud. The matrix is then used to transform the point cloud to a canonical\n","    pose. It consists of two parts: a convolutional network and a fully connected network.\n","    The convolutional network maps the input point cloud to a feature space and the fully connected\n","    network learns the transformation matrix from the feature space.\n","    \"\"\"\n","    def __init__(self, hidden_sizes_conv=[64, 128, 1024], hidden_sizes_fc=[512, 256], k=3):\n","        super().__init__()\n","        self.k=k\n","        self.hidden_sizes_conv=hidden_sizes_conv\n","        self.hidden_sizes_fc=hidden_sizes_fc\n","        \n","        self.conv = self._build_conv()\n","        self.fc = self._build_fc()\n","        self.last_fc = nn.Linear(self.hidden_sizes_fc[-1],self.k*self.k)\n","  \n","    def _build_conv(self):\n","        ########################################################################\n","        # TODO: Build the convolutional network that maps the input point cloud \n","        # to a feature space. The hidden dimension is hidden_sizes_conv\n","        #  \n","        # Hint: construct a series of convolutional layers with batch \n","        # normalization and ReLU activation.\n","        #   The convolution layers follows the following structure:\n","        #   [conv1d]-> [Batch Norm Layer] -> [ReLU]-> [conv1d]-> ...\n","        #   Take the integer in hidden_size_conv for convolution1d layer\n","        #   size and batch_norm layer size\n","        ########################################################################\n","        layers = []\n","        prev_size = self.k\n","        for layer_id, size in enumerate(self.hidden_sizes_conv):\n","            bn = nn.BatchNorm1d(size)\n","            conv = nn.Conv1d(prev_size, size,1)\n","            layers.append(conv)\n","            layers.append(bn)\n","            layers.append(nn.ReLU())\n","            prev_size = size\n","        ################################ END ####################################\n","        return nn.Sequential(*layers)\n","  \n","    def _build_fc(self):\n","        '''\n","        This function implements the \"series mlp\" part in the above graph\n","        The fully connected layers that this function build transforms an input \n","        of shape [bs, 1024] to [bs, k^2] where k is the desired size of the\n","        transformation matrix\n","        '''\n","        layers = []\n","        prev_size = self.hidden_sizes_conv[-1]\n","        for layer_id, size in enumerate(self.hidden_sizes_fc):\n","            bn = nn.BatchNorm1d(size)\n","            fc = nn.Linear(prev_size, size)\n","            layers.append(fc)\n","            layers.append(bn)\n","            layers.append(nn.ReLU())\n","            prev_size = size\n","        return nn.Sequential(*layers)\n","      \n","\n","    def forward(self, input):\n","        ########################################################################\n","        # TODO: Performs the forward pass of the T-Net. \n","        # It first applies the convolutional network to the input point cloud \n","        # to obtain a feature space. Then it applies maxpool along the first dimension\n","        # Then, it applies the fully connected network to the feature space to \n","        # obtain the kxk transformation matrix. Finally, it applies the\n","        # transformation matrix to the input point cloud to transform it to a \n","        # canonical pose.\n","        # input: [batch, k=3, N], N is the points number in a object\n","        # output: [batch, k, k]\n","        # \n","        # Hint: the forward structure is as follows:\n","        # [ConvLayers]->[MaxPooling]->[Flatten]->[Fully Connected Layers]->[theta_Matrix + identity]\n","        # Remember that the identity matrix requires gradient\n","        ########################################################################\n","        # input.shape (bs,n,3)\n","        bs = input.size(0)\n","        \n","        xb = self.conv(input)   \n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        flat = nn.Flatten(1)(pool)\n","        xb = self.fc(flat)\n","      \n","        init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n","        if xb.is_cuda:\n","          init=init.cuda()\n","        matrix = self.last_fc(xb).view(-1,self.k,self.k) + init  \n","        ############################# END ######################################      \n","        return matrix"]},{"cell_type":"markdown","metadata":{"id":"y09ItmEKqa6-"},"source":["Test your T-Net construction is correct.\n","- Test the layer parameter is correct, you should follow the comment in the coding part\n","- Test the T-Net forward is correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3ZAfopBqYBt"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters())\n","\n","torch.manual_seed(42)\n","test_t_net = Tnet()\n","\n","print(\"Getting the T-Net parameters\")\n","if count_parameters(test_t_net)!=803081:\n","    print(\"Error\")\n","    print(\"test_t_net parameters number = \", count_parameters(test_t_net))\n","\n","assert count_parameters(test_t_net)==803081\n","print('passed')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuTWbiYYqjA2"},"outputs":[],"source":["torch.manual_seed(42)\n","x1 = torch.randn(3, 3, 5)\n","\n","y1 = torch.tensor([[[ 1.4712e+00,  1.1447e+00,  6.5780e-02],\n","         [ 2.6862e-01,  1.5355e+00, -7.9635e-01],\n","         [-3.1744e-01,  4.8485e-01,  1.2669e+00]],\n","\n","        [[ 1.0652e+00, -2.9729e-02, -9.1289e-04],\n","         [-2.0753e-01,  1.6646e+00,  5.0989e-01],\n","         [-2.5312e-01,  7.1402e-01,  8.2575e-01]],\n","\n","        [[ 1.3445e+00,  6.7090e-01, -4.4554e-01],\n","         [ 2.4452e-01,  1.1833e+00, -5.8614e-01],\n","         [-5.3094e-02, -1.3413e-01,  9.4217e-01]]])\n","\n","pred_y1 = test_t_net(x1)\n","\n","print(\"Getting T-Net out:\", end = (\" \"))\n","if not torch.allclose(y1, pred_y1, rtol=1e-03, atol=1e-03):\n","    print(\"Error\")\n","    print(\"Your answer is:\", pred_y1)\n","    print(\"The expected answer is:\", y1)\n","\n","assert torch.allclose(y1, pred_y1, rtol=1e-03, atol=1e-03),  \"different y_pred and y\"\n","print(\"passed\")"]},{"cell_type":"markdown","metadata":{"id":"6Kjm_MwVquMj"},"source":["### Transform Class\n","The Transform class is every thing before last MLPs in PointNet. It is a neural network architecture that uses two pairs of spatial transform net (STN) and shared MLP layers to extract global features from a point cloud data of (nx3) shape. The STN is implemented using the T-Net and computes the 3x3 transform matrix, which is then multiplied with the input point cloud to get a transformed point cloud of the same shape. The transformed point cloud is then input into the shared MLP layers along with the feature transform matrix. The output from the shared MLP layers is max pooled along the feature dimension to get a global feature vector. The output also includes the point and feature transform matrices.   \n","\n","![TransformNet](https://github.com/58191554/PointNet-Project/blob/main/img/PointNetStructureFromPaper.png?raw=true)\n","\n","Implement the Transform part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrVvA79Nnnen"},"outputs":[],"source":["class Transform(nn.Module):\n","    def __init__(self, input_size=3, feature_size=64, sharedMLP1_layers=[64, 64], sharedMLP2_layers=[64, 128, 1024], batch_norm = True):\n","        \"\"\"\n","        Transform class is all the pipeline to get a global feature\n","                 _____________________                                     _______________                 ___________________       _______________\n","                |                     |                                   |               |                |                 |     |               |                    \n","        x -->   |   input transform   | --> y (canonical point cloud) --> |  shared MLP   | --> feature -->|feature transform| --> |  shared MLP   | --> max pooling --> z\n","                |_____________________|                                   |_______________|                |_________________|     |  _____________|\n","        The transform class is a neural network architecture that go throught 2 pairs of spactial transform net and shared MLP.\n","        The STN is the T-Net that implement above, and the shared-MLP can be regarded as a one-dimensional convolutional layer.\n","\n","        the input x as a point cloud data of (nx3) shape first compute the 3x3 transform matrix and multiplied with the transform matrix to get a (nx3) transformed point cloud.\n","\n","        the last_activate bool is True when you want to add the last layer with activation function.\n","        \"\"\"\n","        super().__init__()\n","        self.batch_norm = True\n","        \n","        self.input_transform = Tnet(k=3)\n","        self.feature_transform = Tnet(k=64)\n","\n","        self.sharedMLP1 = self._build_sharedMLP(input_size, sharedMLP1_layers, last_activate=True)\n","        self.sharedMLP2 = self._build_sharedMLP(feature_size, sharedMLP2_layers, last_activate=False)\n","\n","    def _build_sharedMLP(self, input_dim, sharedMLP_layers, last_activate = True):\n","        ########################################################################\n","        # TODO: Build the shared MLP layers. Take the sharedMLP_layers list as\n","        # sharedMLP_layers is a list of dimensions of hidden layers\n","        # last_activate represents whether apply activation to the last layer\n","        # hidden dimension in Conv1d, Batch norm\n","        #       The structure is [Conv1d]->[Batch Norm]->[ReLU]\n","        ########################################################################\n","        layers = []\n","        prev_size = input_dim\n","        for layer_id, size in enumerate(sharedMLP_layers):\n","            layers.append(nn.Conv1d(prev_size, size, 1))\n","\n","            if self.batch_norm:\n","                layers.append(nn.BatchNorm1d(size))\n","\n","            if (layer_id < len(sharedMLP_layers)-1) or last_activate:\n","                layers.append(nn.ReLU())\n","\n","            prev_size = size\n","        ########################################################################\n","        return nn.Sequential(*layers)\n","       \n","    def forward(self, input):     #input:[batch_size, 3, 1024] output:[batch_size, 1024]\n","        ########################################################################\n","        # TODO: Implement the code to multiply the transform matrix and the point\n","        # cloud. \n","        #   The transformed x should be the same shape of x [batch_size, 3, N]\n","        #   The transformed feature is [batchsize, 64, N]\n","        #   The maxpooled feature is [batch_size, 1024, 1]\n","        # Hint: \n","        # 1. Get the transform matrix [batch_size, 3, 3] by the T-Net \n","        # 2. Batch matrix multiply the input x and transform matrix\n","        # 3. Input the data into the Shared MLP\n","        # 4. Batch matrix multiply the feature and the feature_transform matrix\n","        # 5. Input the output into the Shared MLP with feature dimension\n","        # 6. Maxpooling along the feature dimension\n","        # 7. output the output data, points transform matrix, and the feature\n","        #       transform matrix\n","        ########################################################################\n","        matrix3x3 = self.input_transform(input)     #[batch_size, 3, 3]\n","        # batch matrix multiplication\n","        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)     #[batch_size, 3, N]\n","        xb = self.sharedMLP1(xb)\n","\n","        matrix64x64 = self.feature_transform(xb)     #[batch_size, 64, 64]\n","        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)     #[batch_size, 64, N]\n","        xb = self.sharedMLP2(xb)\n","\n","        xb = nn.MaxPool1d(xb.size(-1))(xb)     #[batch_size, 1024, 1]\n","        # print(xb.shape)\n","        output = nn.Flatten(1)(xb)     #[batch_size, 1024]\n","        ########################################################################\n","        return output, matrix3x3, matrix64x64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abOOlck6EhRp"},"outputs":[],"source":["# Check the default parameter number to see the model is correct\n","torch.manual_seed(42)\n","test_transform_net = Transform()\n","\n","print(\"Getting the parameter number of the transform net:\", end = (\" \"))\n","test_tranform_net_param_num = count_parameters(test_transform_net)\n","if test_tranform_net_param_num!=2812105:\n","    print(\"Error\")\n","    print(\"Your test_transform_net parameters number = \", count_parameters(test_transform_net))\n","    print(\"Expected answer = 2812105\")\n","    print(\"Difference = \", torch.absolute(test_tranform_net_param_num!=2812105))\n","\n","assert count_parameters(test_transform_net)==2812105\n","print(\"passed\")\n","\n","# Check the forward output is correct\n","torch.manual_seed(42)\n","x2 = torch.randn(2, 3, 5)\n","pred_y2, pred_mat1, pred_mat2 = test_transform_net(x2)\n","\n","mat1 = torch.tensor([[[ 1.0655,  0.4169,  0.1452],\n","         [ 0.0240,  1.7885, -0.6011],\n","         [-0.5582,  0.6857,  1.0256]],\n","\n","        [[ 1.5976,  0.8703, -0.4317],\n","         [ 0.2260,  1.2361,  0.0457],\n","         [ 0.0986,  0.0844,  1.0267]]])\n","\n","print(\"Getting the correct forward output : \", end = (\"\"))\n","if not torch.allclose(mat1, pred_mat1, rtol=1e-03, atol=1e-03):\n","    print(\"Error\")\n","    print(\"Your pred_mat1 is \\n\", pred_mat1)\n","    print(\"Expected answer mat1 is \\n\", mat1)\n","    print(\"Difference = \", torch.norm(pred_mat1- mat1))\n","assert torch.allclose(mat1, pred_mat1, rtol=1e-03, atol=1e-03),  \"different pred_mat1 and mat1\"\n","print(\"passed\")"]},{"cell_type":"markdown","metadata":{"id":"dpah_-1tEjVg"},"source":["### PointNet Classifier\n","The following code defines a PyTorch module called PointNet for classifying point cloud data. The PointNet module includes a Transform class, which takes in 3D point cloud data as input and generates global features and transformation matrices. The global features are then passed through a multi-layer perceptron (MLP) to generate scores for classification. The MLP consists of linear layers, batch normalization, ReLU activation, and dropout layers. The PointNet module outputs the logsoftmax of the scores along with the 3x3 and 64x64 transformation matrices generated by the Transform class. The PointNet module can be customized with different layer configurations, batch normalization, and dropout rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BS1l8BNFnqD6"},"outputs":[],"source":["class PointNet(nn.Module):\n","    def __init__(self, sharedMLP1_layers=[64, 64], sharedMLP2_layers=[64, 128, 1024], dropout_rate = 0.3,  classes = 10, batch_norm = True):\n","        \"\"\"\n","        Point Net the whole neural network for the classification of point cloud data\n","                    _________                         ___     \n","        input x--->|Transform|---> global feature--->|MLP|---> scores\n","                   |_________|                       |___|\n","            args:   sharedMLP1_layers is the first shared MLP in Transform class\n","                    sharedMLP2_layers is the second shared MLP in Transorm class\n","                The MLP has the structure of \n","                    [Linear] -> [Batch Norm] -> [ReLU] -> [Dropout] -> [Linear] -> ... -> [Linear] -> [Batch Norm] -> [ReLU] -> [Linear of class size]\n","\n","                \n","        \"\"\"\n","        super().__init__()\n","        self.transform = Transform(input_size=3, feature_size=64, sharedMLP1_layers=sharedMLP1_layers, sharedMLP2_layers=sharedMLP2_layers)\n","        self.batch_norm = batch_norm\n","        self.fc1 = nn.Linear(1024, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, classes)\n","        self.bn1 = nn.BatchNorm1d(512)\n","        self.bn2 = nn.BatchNorm1d(256)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input):\n","        ########################################################################\n","        # TODO: get the output y, 3x3 transform matrix and 64x64 transform\n","        # matrix from self.transform net.\n","        # Then, y->[fc1]->[bn1]->[relu]->[fc2]->[dropout]->[bn2]->[relu]->[fc3]->z\n","        # return logsoftmax(z), 3x3 transform matrix and 64x64 transform matrix\n","        ########################################################################\n","        xb, matrix3x3, matrix64x64 = self.transform(input)\n","        xb = F.relu(self.bn1(self.fc1(xb)))\n","        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n","        output = self.fc3(xb)\n","        ############################ END #######################################\n","        return self.logsoftmax(output), matrix3x3, matrix64x64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9nHZa8XEm6G"},"outputs":[],"source":["# Test your implementation\n","sharedMLP1_layers=[64, 64]\n","sharedMLP2_layers=[64, 128, 1024]\n","\n","torch.manual_seed(42)\n","test_point_net = PointNet(sharedMLP1_layers, sharedMLP2_layers)\n","\n","print(\"Getting the parameter number of the pointnet:\", end = \" \")\n","test_point_net_param_num = count_parameters(test_point_net)\n","if test_point_net_param_num!=3472339:\n","    print(\"Error\")\n","    print(\"Your test_transform_net parameters number = \", count_parameters(test_point_net))\n","    print(\"The expected answer is 3472339\")\n","    print(\"Difference = \", torch.absolute(torch.tensor(test_point_net_param_num-3472339)))\n","\n","assert count_parameters(test_point_net)==3472339\n","print(\"passed\")\n","\n","torch.manual_seed(42)\n","x3 = torch.randn(3, 3, 5)\n","w, pred_mat3x3, pred_matfxf = test_point_net(x3)\n","\n","mat3x3 = torch.tensor([[[ 1.4712e+00,  1.1447e+00,  6.5780e-02],\n","         [ 2.6862e-01,  1.5355e+00, -7.9635e-01],\n","         [-3.1744e-01,  4.8485e-01,  1.2669e+00]],\n","\n","        [[ 1.0652e+00, -2.9729e-02, -9.1289e-04],\n","         [-2.0753e-01,  1.6646e+00,  5.0989e-01],\n","         [-2.5312e-01,  7.1402e-01,  8.2575e-01]],\n","\n","        [[ 1.3445e+00,  6.7090e-01, -4.4554e-01],\n","         [ 2.4452e-01,  1.1833e+00, -5.8614e-01],\n","         [-5.3094e-02, -1.3413e-01,  9.4217e-01]]])\n","print(\"Check the output correctness by mat3x3\", end = (\" \"))\n","if not torch.allclose(mat3x3, pred_mat3x3, rtol=1e-03, atol=1e-03):\n","    print(\"Error\")\n","    print(\"Your pred_mat3x3 is \\n\", pred_mat1)\n","    print(\"The answer mat3x3 is \\n\", mat1)\n","    print(\"Difference = \", torch.norm(pred_mat3x3- mat3x3))\n","assert torch.allclose(pred_mat3x3, mat3x3, rtol=1e-03, atol=1e-03),  \"different pred_mat1 and mat1\"\n","print(\"passed\")"]},{"cell_type":"markdown","metadata":{"id":"A0OA_vYCFapy"},"source":["## Train the model"]},{"cell_type":"markdown","metadata":{"id":"ZHz0FY6pE9Cy"},"source":["### Loss formula\n","One last thing before training the model is defining the loss of this network. Our loss function is formed by two part: softmax classification loss and regularization term:\n","\n","1. Softmax Classification Loss:\n","Use cross entropy loss to calculate softmax classification loss, the formula is following:\n","\n","    $H(p, q) = - \\sum_{x \\in X} p(x) \\cdot log \\ q(x)$\n","\n","2. Regularization Term:\n","Constrain the feature transformation matrix learned by T-net to be close to orthogonal matrix, so the regularization term formula is following:\n","\n"," $\\lVert I - A A^T \\rVert ^2 _F$\n","\n","By combining the above two parts, our overall loss function is:\n","\n","  $ L = H(p, q) + \\alpha \\times ( \\lVert I - A_1 A_1^T \\rVert ^2 _F + \\lVert I - A_2 A_2^T \\rVert ^2 _F )$   \n","\n","$Note: A_1,A_2$ is 3-dimensional and 64-dimensional feature transform matrix formed by T-net\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Te6mbwndUXBA"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvbDSHM5FF0J"},"outputs":[],"source":["class ClsExperiment:\n","    def __init__(self, train_data, val_data, model: nn.Module,\n","                 lr: float, save=True, alpha=None):\n","        self.train_data = train_data\n","        self.val_data = val_data\n","        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","        self.save=save\n","        if alpha==None:\n","          self.alpha=0.0001\n","        else:\n","          self.alpha=alpha\n","        self.model = model.cuda()\n","        self.loss=[]\n","        self.acc=[]\n","\n","    def train(self, epochs):\n","        epoch_iterator = trange(epochs)\n","        # validation\n","        if self.val_data:\n","            self.evaluate(\"Initial \")\n","        for epoch in epoch_iterator: \n","            self.model.train()\n","            running_loss = 0.0\n","            data_iterator = tqdm(self.train_data)\n","            for i, data in enumerate(data_iterator, 0):\n","                inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n","                self.optimizer.zero_grad()\n","\n","                outputs, m3x3, m64x64 = self.model(inputs.transpose(1,2))\n","\n","                loss = self.getloss(outputs, labels, m3x3, m64x64, self.alpha)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                # print statistics\n","                running_loss += loss.item()\n","                if i % 10 == 9:    # print every 10 mini-batches\n","                    self.loss.append(running_loss / 10)\n","                    data_iterator.set_postfix(loss=running_loss / 10)\n","                    running_loss = 0.0\n","            # validation\n","            if self.val_data:\n","                self.evaluate(\"Epoch:{} \".format(epoch+1))\n","            if self.save:\n","                torch.save(self.model.state_dict(), \"save_\"+str(epoch)+\".pth\")\n","\n","    def evaluate(self, ttype):\n","        self.model.eval()\n","        correct = total = 0\n","        with torch.no_grad():\n","            for data in self.val_data:\n","                inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n","                inputs = inputs.transpose(1,2)\n","                outputs, __, __ = self.model(inputs)\n","                num_correct = self.get_num_correct(outputs.data, labels)\n","                total += labels.size(0)\n","                correct += num_correct\n","            val_acc = 100. * correct / total\n","            self.acc.append(val_acc)\n","            print(ttype+'Test accuracy: %d %%' % val_acc)\n","            #epoch_iterator.set_postfix(val_acc=val_acc)\n","\n","    def get_num_correct(self, outputs, labels):\n","          ########################################################################\n","          # TODO: given outputs of the model, and ground truth labels,\n","          # get the number of correct predictions among bs objects\n","          # Inputs: outputs: [bs, 10]\n","          #         labels: [bs]\n","          # Return: the number of correct predictions among bs objects\n","          # hint: torch.max can be helpful\n","          ########################################################################\n","          _, predicted = torch.max(outputs, 1)\n","          num_correct = (predicted == labels).sum()\n","          ########################################################################\n","          return num_correct.item()\n","\n","    def getloss(self, outputs, labels, m3x3, m64x64, alpha):\n","        #################################################################################\n","        # TODO: Get the loss of the model. The loss of the model consists of three parts:\n","        # 1. Cross entropy loss between outputs and labels\n","        # 2. The norm of two T-net transform matrices: I - m3x3^T @ m3x3 and \n","        #                                              I - m64x64^T @ m64x64\n","        # 3. The final overall loss is corssEctropy(y, y_hat) + alpha*(|I-m3x3^T@ m3x3|_2 + |I-m64x64^T@ m64x64|_2) / batch_size\n","        # \n","        # Inputs: outputs: [bs, 10]\n","        #         labels: [bs]\n","        #         m3x3: [bs, 3, 3]\n","        #         m64x64: m3x3: [bs, 64, 64]\n","        # Return: the loss\n","        # \n","        # \n","        # hint: 1. Remember to add an extra parameter (requires_grad=True) when you create \n","        # a new matrix using pytorch if the created matrix is interacted with other matrices\n","        # in the model\n","        #################################################################################\n","        criterion = torch.nn.CrossEntropyLoss()\n","        bs = outputs.size(0)\n","        id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)       # identity matrix repeat batch_size times\n","        id64x64 = torch.eye(64, requires_grad=True).repeat(bs,1,1)\n","        if outputs.is_cuda:\n","            id3x3=id3x3.cuda()\n","            id64x64=id64x64.cuda()\n","        diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))         # Loss: I-A.t@A\n","        diff64x64 = id64x64-torch.bmm(m64x64,m64x64.transpose(1,2))\n","        loss = criterion(outputs, labels) + alpha * (torch.norm(diff3x3)+torch.norm(diff64x64)) / float(bs)\n","        #################################################################################\n","        return loss\n","    \n","    def plt_loss(self):\n","      x1 = range(0, len(self.loss))\n","      y1 = self.loss\n","      plt.plot(x1, y1, 'o-')\n","      plt.title('Train loss vs. epoches')\n","      plt.ylabel('Train loss')\n","      plt.show()\n","\n","    def plt_accuracy(self):\n","      x1 = range(0, len(self.acc))\n","      y1 = self.acc\n","      plt.plot(x1, y1, 'o-')\n","      plt.title('Test accuracy vs. epoches')\n","      plt.ylabel('Tes accuracy')\n","      plt.show()\n","\n","    def get_pred_label(self):\n","      final_preds = []\n","      final_labels = []\n","      with torch.no_grad():\n","          for data in self.val_data:\n","              inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n","              outputs, __, __ = self.model(inputs.transpose(1,2))\n","              _, preds = torch.max(outputs.data, 1)\n","              final_preds += list(preds.cpu().numpy())\n","              final_labels += list(labels.cpu().numpy())\n","      return final_preds, final_labels\n","\n","    def plot_confusion_matrix_(self, cm, classes, normalize, title='Confusion matrix', cmap=plt.cm.Blues):\n","      if normalize:\n","          cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","          print(\"Normalized confusion matrix\")\n","      else:\n","          print('Confusion matrix, without normalization')\n","\n","      plt.figure(figsize=(8,8))\n","      plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","      plt.title(title)\n","      plt.colorbar()\n","      tick_marks = np.arange(len(classes))\n","      plt.xticks(tick_marks, classes, rotation=45)\n","      plt.yticks(tick_marks, classes)\n","\n","      fmt = '.2f' if normalize else 'd'\n","      thresh = cm.max() / 2.\n","      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","          plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","      plt.tight_layout()\n","      plt.ylabel('True label')\n","      plt.xlabel('Predicted label')\n","\n","    def plt_confusion_matrix(self, normalize=True):\n","      preds, labels = self.get_pred_label()\n","      cm = confusion_matrix(labels, preds)\n","      self.plot_confusion_matrix_(cm, list(classes.keys()), normalize)"]},{"cell_type":"markdown","metadata":{"id":"dhG4OO2TnUCs"},"source":["Test your implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cVkMEr1RwKF"},"outputs":[],"source":["torch.manual_seed(99)\n","pointnet_test = PointNet()\n","clsexp_test = ClsExperiment(trainLoader, testLoader, pointnet_test, lr=0.001, save=False)\n","# Testing get_num_correct\n","correct1 = torch.tensor([3])\n","outputs = torch.randn(8, 4)\n","labels = torch.tensor([0,2,1,1,3,2,3,0])\n","res1 = torch.tensor(clsexp_test.get_num_correct(outputs, labels))\n","print('Testing get_num_correct:')\n","if not torch.allclose(res1, correct1, rtol=1e-03):\n","  print(\"Error\")\n","  print(\"Your answer is: \", res1)\n","  print(\"The expected answer is: \", correct1)\n","else:\n","  print('passed')\n","\n","# Testing get_loss\n","m3x3 = torch.randn(8, 3,3)\n","m64x64 = torch.randn(8, 64,64)\n","res2 = clsexp_test.getloss(outputs, labels, m3x3, m64x64, alpha = 0.0001)\n","correct2 = torch.tensor(1.4213)\n","print('Testing getloss:')\n","if not torch.allclose(res2, correct2, rtol=1e-03):\n","  print(\"Error\")\n","  print(\"Your answer is: \", res2)\n","  print(\"The expected answer is: \", correct2)\n","else:\n","  print('passed')"]},{"cell_type":"markdown","metadata":{"id":"8SXXakLQS0Nh"},"source":["Fine tuning the hyper-parameters to reach a testing accuray of 75 within 15 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83uzTDyFQbMu"},"outputs":[],"source":["lr=0.005\n","alpha=0.0001\n","sharedMLP1_layers=[64, 64]\n","sharedMLP2_layers=[64, 128, 1024]\n","dropout_rate=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPDQ2LbBnycl"},"outputs":[],"source":["pointnet = PointNet(sharedMLP1_layers, sharedMLP2_layers, dropout_rate)\n","pointnet.to(device)\n","clsexp = ClsExperiment(trainLoader, testLoader, pointnet, lr, False, alpha)\n","clsexp.train(epochs=15)"]},{"cell_type":"markdown","metadata":{"id":"YMhMKlLT13T8"},"source":["Visualize the pointcloud after applying T-net transformation matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1pnoxdYIeYU"},"outputs":[],"source":["data=trainDataset[55]\n","inputs, labels = data['pointcloud'].to(device).float(), data['category']\n","__, matrix, _=pointnet(inputs.view(1, 1024, 3).transpose(1,2))\n","index=random.randint(0, len(data['pointcloud']))\n","inputs = inputs.view(1024, 3).cpu()\n","print(\"Before applying transform matrix visualization:\\n\")\n","pcshow(*inputs.T)\n","print(\"After applying transform matrix visualization:\\n\")\n","after_transform=inputs@matrix[0].cpu()\n","after_transform=after_transform.detach().numpy()\n","pcshow(*after_transform.T)\n","print(matrix[0].T@matrix[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9XcoZMy8yoq"},"outputs":[],"source":["clsexp.plt_loss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_QMUeC-F_NI"},"outputs":[],"source":["clsexp.plt_accuracy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vWohxsOOOVG"},"outputs":[],"source":["clsexp.plt_confusion_matrix()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
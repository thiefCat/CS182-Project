{"cells":[{"cell_type":"markdown","source":["# PointNet Segmentation part\n","This is a homework based on the paper [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)"],"metadata":{"id":"To_TiWydkfrc"}},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"RtRqetyc3eG7"}},{"cell_type":"markdown","source":["### Download the dataset by wget"],"metadata":{"id":"ij5shk8-3iQv"}},{"cell_type":"code","source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1LxpGXSDOI9V5mc5DGuRlOcbulxZoFohC' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1LxpGXSDOI9V5mc5DGuRlOcbulxZoFohC\" -O expert_verified.zip && rm -rf /tmp/cookies.txt\n","!unzip -q expert_verified.zip "],"metadata":{"id":"wiwAMnGF3fkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1yQapyoGEfdBen75MTwh6AD7GRY-UMVZn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1yQapyoGEfdBen75MTwh6AD7GRY-UMVZn\" -O points.zip && rm -rf /tmp/cookies.txt\n","!unzip -q points.zip"],"metadata":{"id":"rSOgVWLz3pEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torch.utils.data.dataset import random_split\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","from tqdm.notebook import trange, tqdm\n","\n","import plotly.graph_objects as go\n","import plotly.express as px"],"metadata":{"id":"PKjytd7uZ4Ym"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the dataset"],"metadata":{"id":"dRof9EYc39wL"}},{"cell_type":"code","source":["def read_pts(file):\n","    verts = np.genfromtxt(file)\n","    #return utils.cent_norm(verts)\n","    return verts\n","\n","def read_seg(file):\n","    verts = np.genfromtxt(file, dtype= (int))\n","    return verts\n","\n","def sample_2000(pts, pts_cat):    \n","    res1 = np.concatenate((pts,np.reshape(pts_cat, (pts_cat.shape[0], 1))), axis= 1)\n","    res = np.asarray(random.choices(res1, weights=None, cum_weights=None, k=2000))\n","    images = res[:, 0:3]\n","    categories = res[:, 3]\n","    categories-=np.ones(categories.shape)\n","    return images, categories"],"metadata":{"id":"Lfj088yn3wmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Data(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, root_dir, valid=False, transform=None):\n","        \n","        self.root_dir = root_dir\n","        self.files = []\n","        self.valid=valid\n","\n","        newdir = root_dir + '/expert_verified/points_label/'\n","\n","        for file in os.listdir(newdir):\n","            if file.find(\"(\")!=-1:\n","                continue\n","            o = {}\n","            o['category'] = newdir + file\n","            o['img_path'] = root_dir + '/points/'+ file.replace('.seg', '.pts')\n","            self.files.append(o)\n","       \n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.files[idx]['img_path']\n","        category = self.files[idx]['category']\n","        with open(img_path, 'r') as f:\n","            image1 = read_pts(f)\n","        with open(category, 'r') as f:  \n","            category1 = read_seg(f)\n","        image2, category2 = sample_2000(image1, category1)\n","        if not self.valid:\n","            theta = random.random()*360\n","        \n","        return {'image': np.array(image2, dtype=\"float32\"), 'category': category2.astype(int)}\n"],"metadata":{"id":"O8y67X2W35FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root_dir=\"/content\"\n","dset = Data(root_dir , transform=None)\n","train_num = int(len(dset) * 0.95)\n","val_num = int(len(dset) *0.05)\n","if int(len(dset)) - train_num -  val_num >0 :\n","    train_num = train_num + 1\n","elif int(len(dset)) - train_num -  val_num < 0:\n","    train_num = train_num -1\n","train_dataset, val_dataset = random_split(dset, [train_num, val_num])\n","val_dataset.valid=True\n","\n","print('######### Dataset class created #########')\n","print('Number of images: ', len(dset))\n","print('Sample image shape: ', dset[0]['image'].shape)\n","\n","Seg_train_loader = DataLoader(dataset=train_dataset, batch_size=32)\n","Seg_val_loader = DataLoader(dataset=val_dataset, batch_size=32)"],"metadata":{"id":"sRFE5sbE4ECz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize item in dataset"],"metadata":{"id":"UmrLsA4Oa6BI"}},{"cell_type":"code","source":["def visualize_rotate(data):\n","    x_eye, y_eye, z_eye = 1.25, 1.25, 0.8\n","    frames=[]\n","\n","    def rotate_z(x, y, z, theta):\n","        w = x+1j*y\n","        return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z\n","\n","    for t in np.arange(0, 10.26, 0.1):\n","        xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)\n","        frames.append(dict(layout=dict(scene=dict(camera=dict(eye=dict(x=xe, y=ye, z=ze))))))\n","    fig = go.Figure(data=data,\n","                    layout=go.Layout(\n","                        updatemenus=[dict(type='buttons',\n","                                    showactive=False,\n","                                    y=1,\n","                                    x=0.8,\n","                                    xanchor='left',\n","                                    yanchor='bottom',\n","                                    pad=dict(t=45, r=10),\n","                                    buttons=[dict(label='Play',\n","                                                    method='animate',\n","                                                    args=[None, dict(frame=dict(duration=50, redraw=True),\n","                                                                    transition=dict(duration=0),\n","                                                                    fromcurrent=True,\n","                                                                    mode='immediate'\n","                                                                    )]\n","                                                    )\n","                                            ]\n","                                    )\n","                                ]\n","                    ),\n","                    frames=frames\n","            )\n","\n","    return fig"],"metadata":{"id":"6I8lCHRxaIIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segpcshow(xs,ys,zs, category):\n","    data=[go.Scatter3d(x=xs, y=ys, z=zs,\n","                                   mode='markers')]\n","    fig = visualize_rotate(data)\n","    fig.update_traces(marker=dict(size=2,\n","                      line=dict(width=2,\n","                      color=category)),\n","                      selector=dict(mode='markers'))\n","    fig.show()"],"metadata":{"id":"JSS5OK-TZGPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["item_visualize = train_dataset[1000] # feel free to change the index to visualize different pointcloud\n","pointcloud = item_visualize['image']\n","category = item_visualize['category']\n","segpcshow(*pointcloud.T, category)"],"metadata":{"id":"-KaSE5F_ZUUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"],"metadata":{"id":"y_DgPiM4SIRx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Sturcture"],"metadata":{"id":"11O_HcHQ4Nvy"}},{"cell_type":"markdown","source":["### Segmentation Model\n","\n","**Refer the Supplementary C** in [PointNet paper](https://arxiv.org/abs/1612.00593)\n","\n","The output from the classification global transform section forms a vector [f1 , . . . , fK ], which is a global signature of the input set. We can easily train a SVM or multi-layer perceptron classifier on the shape global features for classification. However, point segmentation requires a combination of local and global knowledge. We implement this by using U-Net-Like structure, concatenate the global and local feature together. The feature in every layer is stacked together to form point feature for every point. The architecture can be seen in the following figure.\n","The architecture can be seen in the following figure.\n","\n","![pointnet_segmentation](https://raw.githubusercontent.com/58191554/PointNet-Project/main/Segmentation.pic.jpg)\n","\n","\n","To get started with implementing PointNet segmentation, we need to change ```Transform``` to ```SegTransform```, ```PointNet``` to ```PointNetSeg``` in order to correspond with the paper implementation.\n","\n","To accelerate training, we limit the segmentation dataset to only one category(plane). So we simplify the segmentation model by not concentrating the one-hot vector to point feature. As a result, the resulting point feature will only have 3008 dimensions.\n","\n","HINT: use `Conv1d` to do shared MLP"],"metadata":{"id":"I5JWz1sm4R_B"}},{"cell_type":"markdown","source":["### T-net part is same as T-net in PointNet classification"],"metadata":{"id":"OUeap6AglDjd"}},{"cell_type":"code","source":["class Tnet(nn.Module):\n","    \"\"\"\n","    T-Net is a type of spatial transformer network (STN) that learns a kxk transformation matrix\n","    for a given point cloud. The matrix is then used to transform the point cloud to a canonical\n","    pose. It consists of two parts: a convolutional network and a fully connected network.\n","    The convolutional network maps the input point cloud to a feature space and the fully connected\n","    network learns the transformation matrix from the feature space.\n","    \"\"\"\n","    def __init__(self, hidden_sizes_conv=[64, 128, 1024], hidden_sizes_fc=[512, 256], k=3):\n","        super().__init__()\n","        self.k=k\n","        self.hidden_sizes_conv=hidden_sizes_conv\n","        self.hidden_sizes_fc=hidden_sizes_fc\n","        self.fc1 = nn.Linear(hidden_sizes_fc[-1],k*k)\n","        self.conv = self._build_conv()\n","        self.fc = self._build_fc()\n","  \n","    def _build_conv(self):\n","        layers = []\n","        prev_size = self.k\n","        for layer_id, size in enumerate(self.hidden_sizes_conv):\n","            bn = nn.BatchNorm1d(size)\n","            conv = nn.Conv1d(prev_size, size,1)  #share mlp can be implemented by three 1*1 convolution filter\n","            layers.append(conv)\n","            layers.append(bn)\n","            layers.append(nn.ReLU())\n","            prev_size = size\n","        return nn.Sequential(*layers)\n","  \n","    def _build_fc(self):\n","\n","        layers = []\n","        prev_size = self.hidden_sizes_conv[-1]\n","        for layer_id, size in enumerate(self.hidden_sizes_fc):\n","            bn = nn.BatchNorm1d(size)\n","            fc = nn.Linear(prev_size, size)\n","            layers.append(fc)\n","            layers.append(bn)\n","            layers.append(nn.ReLU())\n","            prev_size = size\n","        return nn.Sequential(*layers)\n","      \n","\n","    def forward(self, input):\n","\n","        # input.shape (bs,n,3)\n","        bs = input.size(0)\n","        \n","        xb = self.conv(input)   #3->64->128->1024\n","        #torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n","        #bs*n*1024->bs*1024*1\n","        # print(xb.shape)\n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        # print(pool.shape)\n","        \n","        flat = nn.Flatten(1)(pool)   #flatten the tensor bs*1*1024->bs*1024 \n","        #fully connected layer\n","        #1024->512->256\n","        xb = self.fc(flat)\n","      \n","        #initialize with identity matrix bs*k*k\n","        init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n","        if xb.is_cuda:\n","          init=init.cuda()\n","        #b*256->b*(k*k)->b*k*k + init\n","        matrix = self.fc1(xb).view(-1,self.k,self.k) + init\n","        #input b*n*k\n","        return matrix"],"metadata":{"id":"G5sEVXxvg_W8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SegTransform, PointNetSeg part"],"metadata":{"id":"fEjU-jEDlLfp"}},{"cell_type":"code","source":["class SegTransform(nn.Module):\n","    \"\"\"\n","    SegTransform is a class transform point cloud into point feature. When the data go through\n","    the network, the features generated by every layer in the network stack together to form the \n","    point features.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.input_transform = Tnet(k=3)\n","        self.feature_transform = Tnet(k=128)\n","        self.fc1 = nn.Conv1d(3,64,1)\n","        self.fc2 = nn.Conv1d(64,128,1) \n","        self.fc3 = nn.Conv1d(128,128,1)\n","        self.fc4 = nn.Conv1d(128,512,1)\n","        self.fc5 = nn.Conv1d(512,2048,1)\n","\n","        \n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.bn3 = nn.BatchNorm1d(128)\n","        self.bn4 = nn.BatchNorm1d(512)\n","        self.bn5 = nn.BatchNorm1d(2048)\n","\n","    def forward(self, input):\n","        \"\"\"\n","        input: bs*3*n tensor\n","        output:\n","          outs: List of tensor, contain every output of the fc layer\n","          matrix3x3: bs*3*3 matrix output by t-net\n","          matrix128x128: bs*128*128 matrix output by t-net\n","        \"\"\"\n","        #\n","        n_pts = input.size()[2]\n","        outs = []\n","        matrix3x3 = self.input_transform(input)\n","        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n","        out1 = F.relu(self.bn1(self.fc1(xb)))\n","        outs.append(out1)\n","        ########################################################################\n","        # TODO: Performs the forward pass of the SegTransform. \n","        # It is used to generate the point feature that is then feeded into the final MLP part.\n","\n","        # Hint: Make sure adding batch normarlization layer and relu after every fc layer.\n","        # Hint: The final return output should be like [out1, out2, out3...].\n","        # Hint: In order to multiply the t-net output with the input, you need to adjust the shape of the input using torch.transpose function.\n","        # Hint: The global feature (output after maxpool) is repeated n times to concatenate to every local feature. \n","        #       You may find torch.repeat and torch.transpose useful here.\n","        ########################################################################\n","        #bs*64*n -> bs*128*n\n","        out2 = ...\n","        outs.append(out2)\n","        out3 = ...\n","        # 128->128\n","        outs.append(out3)\n","        #bs*128*128\n","        matrix128x128 = ...\n","        #bs*n*128 * bs*128*128 -> bs*n*128 -> bs*128*n\n","        out4 = ...\n","        outs.append(out4)\n","        # 128->512\n","        out5 = ...\n","        outs.append(out5)\n","        #512->2048\n","        xb = ...\n","        #bs*2048*n -> bs*2048*1\n","        xb = ...\n","        #bs*2048->n*bs*2048->2048*bs*n->bs*2048*n\n","        out6 = ...\n","        outs.append(out6)\n","        ########################################################################\n","\n","        return outs, matrix3x3, matrix128x128\n","\n","\n","class PointNetSeg(nn.Module):\n","    \"\"\"\n","    PointNetSeg is a class transform point feature into scores of each point\n","    In this dataset the category number is 4\n","    \"\"\"\n","    def __init__(self, hidden_sizes_fc=[256, 256, 128, 4]):\n","        super().__init__()\n","        self.transform = SegTransform()\n","        self.hidden_sizes_fc=hidden_sizes_fc\n","        self.fc = self._build_fc()\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","        \n","    def _build_fc(self):\n","        layers = []\n","        prev_size = 3008\n","        for layer_id, size in enumerate(self.hidden_sizes_fc):\n","            bn = nn.BatchNorm1d(size)\n","            fc = nn.Conv1d(prev_size, size, 1)\n","            layers.append(fc)\n","            layers.append(bn)\n","            layers.append(nn.ReLU())\n","            prev_size = size\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, input):\n","        \"\"\"\n","        input: bs*3*n tensor\n","        output:\n","          out: logsoftmax score of each point, of shape bs*category_number*n\n","          matrix3x3: bs*3*3 matrix output by t-net\n","          matrix128x128: bs*128*128 matrix output by t-net\n","        \"\"\"\n","        inputs, matrix3x3, matrix128x128 = self.transform(input)\n","        #bs*3008*n\n","        stack = torch.cat(inputs,1)\n","        output = self.fc(stack)\n","        \n","        return self.logsoftmax(output), matrix3x3, matrix128x128"],"metadata":{"id":"slLjnm7a4ZwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test SegTransform, PointNetSeg"],"metadata":{"id":"RFyD4PbglbWJ"}},{"cell_type":"code","source":["_set_seed(2017)\n","model = SegTransform()\n","\n","if count_parameters(model)!=6970057:\n","    print(\"Error\")\n","    print(\"test_t_net parameters number = \", count_parameters(model))\n","\n","assert count_parameters(model)==6970057\n","\n","x1 = torch.randn(3, 3, 5)\n","\n","y1=torch.tensor([0.1483, 0.0000, 1.2474, 0.6692, 0.8206, 0.0000, 0.9475, 0.0000, 0.0000, 0.4241])\n","\n","pred_y1, _, _ = model(x1)\n","assert torch.allclose(y1, pred_y1[0].view(-1)[35:45], rtol=1e-03, atol=1e-03),  \"different y_pred and y\"\n","print(\"SegTransform test pass!\")"],"metadata":{"id":"siAwuhpIVpcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_set_seed(2017)\n","model = PointNetSeg()\n","\n","if count_parameters(model)!=7840853:\n","    print(\"Error\")\n","    print(\"test_t_net parameters number = \", count_parameters(model))\n","\n","assert count_parameters(model)==7840853\n","\n","x1 = torch.randn(3, 3, 5)\n","\n","y1=torch.tensor([-1.8602, -1.3863, -1.5023, -2.0664, -2.1121, -1.3051, -2.0492, -0.6164, -1.7983, -1.9253])\n","\n","pred_y1, _, _ = model(x1)\n","assert torch.allclose(y1, pred_y1.view(-1)[35:45], rtol=1e-03, atol=1e-03),  \"different y_pred and y\"\n","print(\"PointNetSeg test pass!\")"],"metadata":{"id":"n6TM6c6fWTdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training loop for segmentation"],"metadata":{"id":"rYj2NhFu4pxz"}},{"cell_type":"code","source":["class ExperimentSeg:\n","    def __init__(self, train_data, val_data, model: nn.Module,\n","                 lr: float, save=True):\n","        self.train_data = train_data\n","        self.val_data = val_data\n","        self.model = model.cuda()\n","        self.optimizer = torch.optim.Adam(pointnet.parameters(), lr=lr)\n","        self.save=save\n","        self.loss=[]\n","        self.acc=[]\n","\n","    def train(self, epochs):\n","        epoch_iterator = trange(epochs)\n","        # validation\n","        if self.val_data:\n","            self.evaluate(\"Initial \")\n","        for epoch in epoch_iterator: \n","            self.model.eval()\n","            correct = total = 0\n","            self.model.train()\n","            running_loss = 0.0\n","            data_iterator = tqdm(self.train_data)\n","            for i, data in enumerate(data_iterator, 0):\n","                inputs, labels = data['image'].to(device), data['category'].to(device)\n","                self.optimizer.zero_grad()\n","                outputs, m3x3, m64x64 = self.model(inputs.transpose(1,2))\n","\n","                loss = self.getloss(outputs, labels, m3x3, m64x64)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                # print statistics\n","                running_loss += loss.item()\n","                if i % 10 == 9:    # print every 10 mini-batches\n","                    self.loss.append(running_loss / 10)\n","                    data_iterator.set_postfix(loss=running_loss / 10)\n","                    running_loss = 0.0\n","                    \n","            # validation\n","            if self.val_data:\n","                self.evaluate(\"Epoch:{} \".format(epoch+1))\n","\n","            # save the model\n","            if self.save:\n","                torch.save(self.model.state_dict(), \"save_\"+str(epoch)+\".pth\")\n","\n","    def evaluate(self, ttype):\n","        self.model.eval()\n","        correct = total = 0\n","        with torch.no_grad():\n","            for data in self.val_data:\n","                inputs, labels = data['image'].to(device).float(), data['category'].to(device)\n","                outputs, __, __ = self.model(inputs.transpose(1,2))\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0) * labels.size(1)\n","                correct += (predicted == labels).sum().item()\n","            val_acc = 100. * correct / total\n","            self.acc.append(val_acc)\n","            print(ttype+'Valid accuracy: %d %%' % val_acc)\n","            #epoch_iterator.set_postfix(val_acc=val_acc)\n","            \n","    def getloss(self, outputs, labels, m3x3, m128x128, alpha = 0.0001):\n","        criterion = torch.nn.NLLLoss()\n","        bs=outputs.size(0)\n","        id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n","        id128x128 = torch.eye(128, requires_grad=True).repeat(bs,1,1)\n","        if outputs.is_cuda:\n","            id3x3=id3x3.cuda()\n","            id128x128=id128x128.cuda()\n","        diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n","        diff128x128 = id128x128-torch.bmm(m128x128,m128x128.transpose(1,2))\n","        return criterion(outputs, labels) + alpha * (torch.norm(diff3x3)+torch.norm(diff128x128)) / float(bs)\n","    \n","    def plt_loss(self):\n","      x1 = range(0, len(self.loss))\n","      y1 = self.loss\n","      plt.plot(x1, y1, 'o-')\n","      plt.title('Test loss vs. epoches')\n","      plt.ylabel('Test loss')\n","      plt.show()   \n","\n","    def plt_accuracy(self):\n","      x1 = range(0, len(self.acc))\n","      y1 = self.acc\n","      plt.plot(x1, y1, 'o-')\n","      plt.title('Validation accuracy vs. epoches')\n","      plt.ylabel('Validation accuracy')\n","      plt.show()\n","  \n","    def visualize_pred_label(self):\n","      for data in self.val_data:\n","          inputs, labels = data['image'].to(device), data['category'].to(device)\n","          outputs, __, __ = self.model(inputs.transpose(1,2))\n","          _, predicted = torch.max(outputs.data, 1)\n","          index=random.randint(0, len(data['image']))\n","          inputs, labels, predicted = inputs.cpu(), labels.cpu(), predicted.cpu()\n","          print(\"True Label visualization:\\n\")\n","          segpcshow(*inputs[index].T, labels[index])\n","          print(\"Predicted Label visualization:\\n\")\n","          segpcshow(*inputs[index].T, predicted[index])\n","          break"],"metadata":{"id":"HkIFyGAE4maZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"NX_NsZ9EbZAB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pointnet = PointNetSeg()\n","pointnet.to(device)\n","segexp = ExperimentSeg(Seg_train_loader, Seg_val_loader, pointnet, 0.001, save=False)\n","segexp.train(5)"],"metadata":{"id":"SuXUtI1q5BjI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize the result"],"metadata":{"id":"2lksHU5elwXG"}},{"cell_type":"code","source":["segexp.plt_loss()"],"metadata":{"id":"4qXjVWGg5M26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segexp.plt_accuracy()"],"metadata":{"id":"gRe4IhNN5M9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segexp.visualize_pred_label()"],"metadata":{"id":"YTbuKiTVjBcm"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}